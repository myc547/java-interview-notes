
---
## Kafka 高级面试题
# Kafka 高级面试题

## 题目

**题目：**
请详细说明 Kafka 的 ISR、HW、LEO 机制及其作用。

**答案：**
**核心概念：**
```
┌─────────────────────────────────────────────────────────────────┐
│                    Kafka 复制机制核心概念                        │
├─────────────────────────────────────────────────────────────────┤
│                                                              │
│  LEO（Log End Offset）                                        │
│  - 每个副本的最后一个消息偏移量                                 │
│  - 代表副本的写入位置                                           │
│                                                              │
│  HW（High Watermark）                                         │
│  - 消费者能消费到的最大偏移量                                   │
│  - 消息可见性的分界点                                          │
│                                                              │
│  ISR（In-Sync Replicas）                                      │
│  - 与 Leader 保持同步的副本集合                                │
│  - 参与 Leader 选举                                           │
│                                                              │
└─────────────────────────────────────────────────────────────────┘
```

**LEO（Log End Offset）：**
```java
// 每个分区副本都有自己的 LEO
// 表示该副本下一条消息应该写入的位置

// 副本数据结构
class Replica {
    long leo;           // 当前 LEO
    long hw;            // 当前 HW
    List<Long> log;     // 消息日志
}

// 写入消息时更新 LEO
void appendMessage(Message message) {
    // 消息追加到日志
    log.add(message);
    
    // 更新 LEO
    leo = log.size();
}
```

**HW（High Watermark）：**
```
分区消息分布：
┌──────────────────────────────────────────────────────────────────┐
│  Offset:  0   1   2   3   4   5   6   7   8   9  10  11  12     │
│           ───────────────────────────────────────────────────── │
│  Leader:  [M0][M1][M2][M3][M4][M5][M6][M7]                       │
│           │            │        ▲                              │
│           │            │        │                              │
│           │            │     HW=8                             │
│           │            │                                      │
│  Follower: [M0][M1][M2][M3]                                    │
│                      ▲                                         │
│                      │                                         │
│                   LEO=4                                        │
└──────────────────────────────────────────────────────────────────┘

消费者可见消息：0 ~ 7（M0 ~ M7）
消费者不可见消息：8 ~ 12

HW 更新规则：
1. Leader 收到生产者消息，先更新自己的 LEO
2. Follower 拉取消息后，更新自己的 LEO
3. Follower 发送 FetchRequest 时，带上自己的 LEO
4. Leader 取所有 ISR 副本的最小 LEO 作为 HW
```

**ISR（In-Sync Replicas）：**
```java
// ISR 判定条件
class ReplicaManager {
    
    // 判断副本是否在 ISR 中
    boolean isInSync(Replica replica) {
        // 条件1：副本在分区的副本列表中
        if (!partition.replicas.contains(replica)) {
            return false;
        }
        
        // 条件2：副本与 Leader 的差距在配置范围内
        long lag = replica.leo - partition.hw;
        if (lag > config.replica.lag.max.messages) {
            return false;
        }
        
        // 条件3：副本的最后 fetch 时间在配置范围内
        long timeSinceLastFetch = 
            currentTime - replica.lastFetchTime;
        if (timeSinceLastFetch > config.replica.fetch.timeout.ms) {
            return false;
        }
        
        return true;
    }
    
    // 更新 ISR 列表
    void updateISR(Partition partition) {
        List<Replica> newISR = new ArrayList<>();
        
        for (Replica replica : partition.replicas) {
            if (isInSync(replica)) {
                newISR.add(replica);
            }
        }
        
        partition.isr = newISR;
        
        // ISR 变化时更新 Zookeeper
        if (!newISR.equals(partition.isr)) {
            zkClient.updateISR(partition.topic, partition.partitionId, newISR);
        }
    }
}
```

**HW 更新流程：**
```
┌─────────────────────────────────────────────────────────────────┐
│                    HW 更新流程                                   │
│                                                              │
│  1. Leader 副本收到消息                                        │
│     ├── 消息写入 Leader 本地日志                               │
│     └── 更新 Leader LEO                                        │
│                                                              │
│  2. Follower 拉取消息                                         │
│     ├── Follower 发送 FetchRequest                             │
│     ├── 请求中包含 Follower 的 LEO                             │
│     └── Leader 返回消息                                        │
│                                                              │
│  3. Leader 处理 FetchRequest                                  │
│     ├── 遍历 ISR 副本列表                                      │
│     ├── 收集所有副本的 LEO                                     │
│     └── HW = min(ISR 中所有副本的 LEO)                         │
│                                                              │
│  4. Leader 响应 FetchRequest                                  │
│     ├── 返回消息（从 LEO 到 HW）                               │
│     └── 包含更新的 HW                                          │
│                                                              │
│  5. Follower 更新                                             │
│     ├── 消息写入本地日志                                       │
│     └── 更新 Follower LEO 和 HW                                │
│                                                              │
└─────────────────────────────────────────────────────────────────┘
```

**ISR 与数据可靠性：**
```java
// acks 配置与 ISR 的关系

// acks=1
// Leader 写入成功即返回，不等待 ISR
// 风险：Leader 崩溃后，未同步到 ISR 的消息丢失

// acks=all（或 -1）
// Leader 等待 ISR 中所有副本同步成功
// 可靠性高，性能低

// acks=-1, min.insync.replicas=2
// 至少等待 ISR 中 2 个副本同步

// 配置示例
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "all");  // 等待所有 ISR 副本
props.put("min.insync.replicas", "2");  // 最少 2 个 ISR
props.put("retries", 3);
```

**ISR 变化触发 Leader 选举：**
```java
// ISR 变化监听
class ISRChangeListener {
    
    void onISRChange(Partition partition, List<Replica> newISR) {
        // 1. 如果 Leader 不在新的 ISR 中
        if (!newISR.contains(partition.leader)) {
            // 触发 Leader 选举
            electNewLeader(partition, newISR);
        }
        
        // 2. 更新 Zookeeper
        zkClient.writeISR(partition.topic, partition.partitionId, newISR);
    }
    
    void electNewLeader(Partition partition, List<Replica> isr) {
        if (isr.isEmpty()) {
            // ISR 为空，不能选举，等待
            return;
        }
        
        // 从 ISR 中选择新的 Leader
        // 优先选择 AR（Assigned Replicas）中顺序最靠前的
        Replica newLeader = selectLeaderFromISR(partition, isr);
        
        // 更新 Leader
        partition.leader = newLeader;
        
        // 发送 LeaderAndIsr 请求
        partition.brokerList.forEach(broker -> {
            sendLeaderAndIsrRequest(broker, partition, newLeader);
        });
    }
}
```

---

## 2. Kafka 分区分配策略？

**题目：**
Kafka 有哪些分区分配策略？分别适用于什么场景？

**答案：**
**分区分配策略分类：**
```
┌─────────────────────────────────────────────────────────────────┐
│                    Kafka 分区分配策略                            │
├─────────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────────┐    ┌──────────────────┐                │
│  │   RangeAssignor  │    │ RoundRobinAssignor│               │
│  │   （范围分配）    │    │   （轮询分配）    │                │
│  └──────────────────┘    └──────────────────┘                │
│                                                              │
│  ┌──────────────────┐    ┌──────────────────┐                │
│  │ StickyAssignor  │    │ CooperativeSticky│               │
│  │   （粘性分配）    │    │   （协作粘性）    │                │
│  └──────────────────┘    └──────────────────┘                │
│                                                              │
└─────────────────────────────────────────────────────────────────┘
```

**RangeAssignor（范围分配）：**
```java
// 原理：按主题分配，每个主题独立计算

// 示例：2 个消费者，3 个主题，每个主题 3 个分区
// 主题 A：分区 A-0, A-1, A-2
// 主题 B：分区 B-0, B-1, B-2
// 主题 C：分区 C-0, C-1, C-2

// 计算公式：
// n = 分区数 / 消费者数
// m = 分区数 % 消费者数
// 前 m 个消费者分配 n+1 个分区
// 剩余消费者分配 n 个分区

// 分配结果：
// Consumer0: A-0, A-1, B-0, B-1, C-0, C-1  (每个主题 2 个)
// Consumer1: A-2, B-2, C-2                 (每个主题 1 个)

// 实现代码
class RangeAssignor implements PartitionAssignor {
    
    Map<String, List<MemberSubscription>> subscriptions;
    
    @Override
    public Map<String, List<TopicPartition>> assign(
            Map<String, Integer> partitionsPerTopic,
            Map<String, List<String>> subscriptions) {
        
        Map<String, List<TopicPartition>> assignment = 
            new HashMap<>();
        
        // 每个消费者初始化
        for (String memberId : subscriptions.keySet()) {
            assignment.put(memberId, new ArrayList<>());
        }
        
        // 遍历每个主题
        for (Map.Entry<String, List<String>> entry : subscriptions.entrySet()) {
            String topic = entry.getKey();
            List<String> consumers = entry.getValue();
            int numPartitions = partitionsPerTopic.getOrDefault(topic, 0);
            
            // 按范围分配
            Collections.sort(consumers);
            int numConsumersPerTopic = consumers.size();
            int n = numPartitions / numConsumersPerTopic;
            int m = numPartitions % numConsumersPerTopic;
            
            for (int i = 0; i < numConsumersPerTopic; i++) {
                int start = n * i + Math.min(i, m);
                int length = n + (i < m ? 1 : 0);
                
                for (int j = 0; j < length; j++) {
                    int partition = start + j;
                    String consumer = consumers.get(i);
                    assignment.get(consumer)
                        .add(new TopicPartition(topic, partition));
                }
            }
        }
        
        return assignment;
    }
}
```

**RoundRobinAssignor（轮询分配）：**
```java
// 原理：将所有主题的分区打散，依次分配给消费者

// 示例：2 个消费者，3 个主题，每个主题 3 个分区
// 所有分区序列：[A-0, A-1, A-2, B-0, B-1, B-2, C-0, C-1, C-2]

// 轮询分配：
// Consumer0: A-0, B-0, C-0, A-2, B-2
// Consumer1: A-1, B-1, C-1, A-1, B-1  (重复的分区？不对)

// 正确实现：
class RoundRobinAssignor implements PartitionAssignor {
    
    @Override
    public Map<String, List<TopicPartition>> assign(
            Map<String, Integer> partitionsPerTopic,
            Map<String, List<String>> subscriptions) {
        
        Map<String, List<TopicPartition>> assignment = 
            new HashMap<>();
        
        // 初始化
        for (String memberId : subscriptions.keySet()) {
            assignment.put(memberId, new ArrayList<>());
        }
        
        // 收集所有分区
        List<TopicPartition> allPartitions = new ArrayList<>();
        for (Map.Entry<String, Integer> entry : partitionsPerTopic.entrySet()) {
            String topic = entry.getKey();
            int numPartitions = entry.getValue();
            for (int i = 0; i < numPartitions; i++) {
                allPartitions.add(new TopicPartition(topic, i));
            }
        }
        
        // 打散分区（随机排序）
        Collections.shuffle(allPartitions);
        
        // 轮询分配
        List<String> consumerList = new ArrayList<>(subscriptions.keySet());
        int consumerIndex = 0;
        
        for (TopicPartition partition : allPartitions) {
            String consumer = consumerList.get(consumerIndex);
            assignment.get(consumer).add(partition);
            
            // 轮询到下一个消费者
            consumerIndex = (consumerIndex + 1) % consumerList.size();
        }
        
        return assignment;
    }
}
```

**StickyAssignor（粘性分配）：**
```java
// 原理：尽可能保持原有的分配结果不变，只移动必要的分区

// 目标：
// 1. 尽可能均匀分配
// 2. 尽可能少地移动分区

// 使用场景：分区再平衡时，减少消费者重新分配的开销

class StickyAssignor implements PartitionAssignor {
    
    @Override
    public Map<String, List<TopicPartition>> assign(
            Map<String, Integer> partitionsPerTopic,
            Map<String, List<String>> subscriptions) {
        
        // 1. 使用 RoundRobin 获得初始分配
        Map<String, List<TopicPartition>> assignment = 
            new RoundRobinAssignor().assign(
                partitionsPerTopic, subscriptions);
        
        // 2. 优化分配（粘性）
        // 尽可能保持原有分配
        List<TopicPartition> unassigned = new ArrayList<>();
        
        // 3. 调整分配使其均匀
        while (!isBalanced(assignment)) {
            // 找到分配不均的消费者
            String overAssigned = findOverAssignedConsumer(assignment);
            String underAssigned = findUnderAssignedConsumer(assignment);
            
            if (overAssigned == null || underAssigned == null) {
                break;
            }
            
            // 从过度分配的消费者移动分区给不足的
            TopicPartition partition = 
                findMovablePartition(assignment, overAssigned);
            
            if (partition == null) {
                break;
            }
            
            // 移动分区
            assignment.get(overAssigned).remove(partition);
            assignment.get(underAssigned).add(partition);
        }
        
        return assignment;
    }
    
    private boolean isBalanced(
            Map<String, List<TopicPartition>> assignment) {
        // 检查是否均匀
        int minSize = Integer.MAX_VALUE;
        int maxSize = Integer.MIN_VALUE;
        
        for (List<TopicPartition> partitions : assignment.values()) {
            minSize = Math.min(minSize, partitions.size());
            maxSize = Math.max(maxSize, partitions.size());
        }
        
        return maxSize - minSize <= 1;
    }
}
```

**CooperativeStickyAssignor（协作粘性分配）：**
```java
// Kafka 2.4+ 引入
// 与 StickyAssignor 类似，但不采用"Stop-The-World"方式
// 而是采用增量再平衡

// 特点：
// 1. 再平衡过程中消费者可以继续消费
// 2. 分批移动分区
// 3. 更平滑的再平衡过程

// 使用方式
Properties props = new Properties();
props.put("partition.assignment.strategy", 
    "org.apache.kafka.clients.consumer.CooperativeStickyAssignor");
```

**策略选择建议：**
```java
// 场景1：多个主题，消费者数量固定
// 推荐：RangeAssignor
// 原因：按主题分配，简单直观

// 场景2：消费者会动态加入/退出
// 推荐：StickyAssignor 或 CooperativeStickyAssignor
// 原因：减少分区移动，降低再平衡开销

// 场景3：需要均匀分配
// 推荐：RoundRobinAssignor
// 原因：完全均匀分配

// Kafka 2.4+ 推荐
// 推荐：CooperativeStickyAssignor
// 原因：支持增量再平衡，性能更好
```

---

## 3. Kafka Exactly Once 语义？

**题目：**
Kafka 如何实现 Exactly Once 语义？

**答案：**
**消息交付语义：**
```
┌─────────────────────────────────────────────────────────────────┐
│                    消息交付语义对比                              │
├─────────────────────────────────────────────────────────────────┤
│                                                              │
│  At Least Once（至少一次）                                     │
│  - 消息不会丢失，但可能重复                                      │
│  - Producer 重试可能导致重复                                     │
│                                                              │
│  At Most Once（至多一次）                                       │
│  - 消息不会重复，但可能丢失                                      │
│  - 不推荐使用                                                  │
│                                                              │
│  Exactly Once（精确一次）                                       │
│  - 消息既不丢失也不重复                                         │
│  - Kafka 0.11+ 支持                                            │
│                                                              │
└─────────────────────────────────────────────────────────────────┘
```

**幂等性实现：**
```java
// 开启幂等性
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("enable.idempotence", true);  // 开启幂等性
props.put("acks", "all");               // 需要配合
props.put("retries", Integer.MAX_VALUE); // 重试次数
props.put("max.in.flight.requests.per.connection", 5);  // 限制

// 幂等性原理

// 1. Producer 初始化时，Broker 分配 Producer ID
//    PID + Producer Epoch
//    PID：生产者唯一标识
//    Producer Epoch：防止旧生产者影响

// 2. 每条消息携带：PID + sequence number

// 3. Broker 端维护（每个分区的）发送窗口
class BrokerPartitionState {
    int partitionId;
    short producerId;      // 生产者 ID
    short producerEpoch;   // 生产者 Epoch
    int lastSequence;      // 最后接收的序列号
    Map<Short, Integer> producerStates;  // 每个生产者最后序列号
    
    // 处理消息
    boolean append(Message message) {
        // 1. 检查生产者 ID 和 Epoch
        if (message.producerId != this.producerId ||
            message.producerEpoch != this.producerEpoch) {
            // 新的生产者或 Epoch 变化，重置状态
            this.producerId = message.producerId;
            this.producerEpoch = message.producerEpoch;
            this.lastSequence = -1;
        }
        
        // 2. 检查序列号（去重）
        int sequence = message.sequence;
        if (sequence <= this.lastSequence) {
            // 重复消息，忽略
            return true;
        }
        
        // 3. 检查是否连续（有序）
        if (sequence != this.lastSequence + 1) {
            // 序列号不连续，拒绝
            return false;
        }
        
        // 4. 追加消息
        log.append(message);
        this.lastSequence = sequence;
        
        return true;
    }
}
```

**事务性实现：**
```java
// 开启事务
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("enable.idempotence", true);
props.put("transactional.id", "my-transactional-id");  // 事务 ID

// 使用事务
KafkaProducer<String, String> producer = 
    new KafkaProducer<>(props);

producer.initTransactions();

try {
    producer.beginTransaction();
    
    // 发送消息到多个主题
    producer.send(new ProducerRecord<>("topic1", "key1", "value1"));
    producer.send(new ProducerRecord<>("topic2", "key2", "value2"));
    
    // 发送消费-生产事务（Stream 场景）
    producer.sendOffsetsToTransaction(
        getOffsets(), 
        consumerGroupId
    );
    
    producer.commitTransaction();
    
} catch (Exception e) {
    producer.abortTransaction();
}

// 事务原理

// 1. 事务协调者（Transaction Coordinator）
//    - 管理事务状态
//    - 协调多个分区的写入

// 2. 事务日志（__transaction_state 主题）
//    - 记录事务状态变化
//    - 用于故障恢复

// 3. 两阶段提交
//    - 第一阶段：PrepareCommit
//    - 第二阶段：Commit
```

**事务状态机：**
```java
// 事务状态
enum TransactionState {
    EMPTY,           // 初始状态
    BEGIN,           // 事务开始
    PREPARE_COMMIT,  // 准备提交
    COMPLETE_COMMIT, // 提交完成
    PREPARE_ABORT,   // 准备回滚
    COMPLETE_ABORT,  // 回滚完成
    EMPTY_ABORT      // 回滚完成（恢复到初始）
}

// 状态流转
EMPTY → BEGIN → PREPARE_COMMIT → COMPLETE_COMMIT → EMPTY
EMPTY → BEGIN → PREPARE_ABORT  → COMPLETE_ABORT  → EMPTY
```

**Stream 中的 Exactly Once：**
```java
// Kafka Streams 精确一次处理

Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-streams-app");
props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, 
    StreamsConfig.EXACTLY_ONCE_V2);  // 精确一次

StreamsBuilder builder = new StreamsBuilder();

// KTable 聚合保证精确一次
KTable<String, Long> counts = builder
    .stream("source-topic")
    .groupBy((key, value) -> value)
    .count(Materialized.as("count-store"));

counts.toStream().to("output-topic");

// 特性：
// 1. 消费和生产事务
// 2. 状态存储的 changelog
// 3. 消息去重
```

** Exactly Once 配置建议：**
```java
// 生产者配置
Properties producerProps = new Properties();
producerProps.put("bootstrap.servers", "localhost:9092");
producerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
producerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// 幂等性和事务
producerProps.put("enable.idempotence", true);
producerProps.put("acks", "all");
producerProps.put("retries", Integer.MAX_VALUE);
producerProps.put("max.in.flight.requests.per.connection", 5);
producerProps.put("transactional.id", "unique-transaction-id");

// 消费者配置
Properties consumerProps = new Properties();
consumerProps.put("bootstrap.servers", "localhost:9092");
consumerProps.put("group.id", "my-consumer-group");
consumerProps.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
consumerProps.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

// 消费者隔离级别
consumerProps.put("isolation.level", "read_committed");

// Streams 配置
Properties streamProps = new Properties();
streamProps.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-app");
streamProps.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
streamProps.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, 
    StreamsConfig.EXACTLY_ONCE_V2);
```

---

## 4. Kafka 消费者组再平衡？

**题目：**
请详细说明 Kafka 消费者组的再平衡机制。

**答案：**
**再平衡触发条件：**
```java
// 触发再平衡的场景：
// 1. 消费者加入组
//    - 新消费者启动
//    - 消费者从故障恢复

// 2. 消费者离开组
//    - 消费者主动退出（close）
//    - 消费者心跳超时（session.timeout.ms）
//    - 消费者处理时间过长（max.poll.interval.ms）

// 3. 分区变化
//    - 主题分区数增加
//    - 消费者组成员订阅的主题变化
```

**再平衡协议：**
```
┌─────────────────────────────────────────────────────────────────┐
│                    再平衡协议流程                                │
│                                                              │
│  ┌─────────┐     ┌─────────┐     ┌─────────┐                  │
│  │ Join    │────>│ Sync    │────>│ Stable  │                  │
│  │ Request │     │ Group   │     │         │                  │
│  └─────────┘     └─────────┘     └─────────┘                  │
│       │              │               │                         │
│       ▼              ▼               ▼                         │
│  所有消费者       Leader 消费者    再平衡完成                   │
│  加入组          计算分配方案      正常消费                      │
│                                                              │
│  消息类型：                                                  │
│  - JoinGroup：消费者请求加入组                               │
│  - SyncGroup：同步分配结果                                   │
│  - Heartbeat：消费者定期发送心跳                             │
│  - LeaveGroup：消费者主动离开组                              │
└─────────────────────────────────────────────────────────────────┘
```

**JoinGroup 阶段：**
```java
// 消费者发送 JoinGroup 请求
class Consumer implements Runnable {
    
    private String memberId;
    private List<String> subscribedTopics;
    
    void joinGroup() {
        JoinGroupRequest request = new JoinGroupRequest();
        request.groupId = consumerGroupId;
        request.memberId = memberId;
        request.protocols = getPartitionAssignor().name();
        request.topics = subscribedTopics;
        
        // 发送到组协调者
        JoinGroupResponse response = 
            coordinator.sendJoinGroupRequest(request);
        
        // 获取分配策略
        String selectedProtocol = response.groupProtocol;
        memberId = response.memberId;
        
        // 如果是 Leader，等待 SyncGroup
        if (response.isLeader) {
            this.leaderId = memberId;
            List<MemberMetadata> allMembers = response.members;
            
            // Leader 计算分配方案
            Map<String, List<TopicPartition>> assignment = 
                assign(allMembers);
            
            storeAssignment(assignment);
        }
    }
}
```

**SyncGroup 阶段：**
```java
// Leader 消费者计算分配方案
class LeaderConsumer {
    
    Map<String, List<TopicPartition>> assign(
            List<MemberMetadata> members) {
        
        // 1. 获取每个消费者订阅的主题
        Map<String, List<String>> subscriptions = new HashMap<>();
        for (MemberMetadata member : members) {
            subscriptions.put(member.memberId, member.topics);
        }
        
        // 2. 获取每个主题的分区数
        Map<String, Integer> partitionsPerTopic = 
            adminClient.getPartitionsForTopics(
                subscriptions.values().stream()
                    .flatMap(List::stream)
                    .distinct()
                    .collect(Collectors.toList()));
        
        // 3. 使用分配策略计算
        PartitionAssignor assignor = 
            PartitionAssignor.getAssignor(selectedProtocol);
        
        Map<String, List<TopicPartition>> assignment = 
            assignor.assign(partitionsPerTopic, subscriptions);
        
        return assignment;
    }
}

// 所有消费者发送 SyncGroup 请求
class Consumer {
    
    void syncGroup() {
        SyncGroupRequest request = new SyncGroupRequest();
        request.groupId = consumerGroupId;
        request.memberId = memberId;
        
        if (isLeader) {
            // Leader 发送分配方案
            request.groupAssignment = 
                getAssignmentMap();
        } else {
            // 普通消费者发送空
            request.groupAssignment = Collections.emptyMap();
        }
        
        // 协调者返回分配结果
        SyncGroupResponse response = 
            coordinator.sendSyncGroupRequest(request);
        
        // 应用分配方案
        List<TopicPartition> assignedPartitions = 
            response.partitions;
        
        // 开始消费
        startConsuming(assignedPartitions);
    }
}
```

**心跳机制：**
```java
// 消费者心跳线程
class HeartbeatThread extends Thread {
    
    private volatile boolean running = true;
    
    @Override
    public void run() {
        while (running) {
            try {
                // 发送心跳
                HeartbeatRequest request = new HeartbeatRequest();
                request.groupId = consumerGroupId;
                request.memberId = memberId;
                request.generationId = generationId;
                
                HeartbeatResponse response = 
                    coordinator.sendHeartbeatRequest(request);
                
                if (response.error == MemberNotExistsError) {
                    // 被踢出组，需要重新 Join
                    coordinator.maybeLeaveGroup();
                    coordinator.joinGroup();
                }
                
                // 等待下次心跳
                Thread.sleep(heartbeatInterval);
                
            } catch (Exception e) {
                // 心跳失败，重试
                handleHeartbeatFailure(e);
            }
        }
    }
}

// 配置参数
Properties props = new Properties();
props.put("heartbeat.interval.ms", 3000);     // 心跳间隔
props.put("session.timeout.ms", 30000);       // 会话超时
props.put("max.poll.interval.ms", 300000);    // 拉取间隔
```

**再平衡监听器：**
```java
// 消费者再平衡监听器
KafkaConsumer<String, String> consumer = 
    new KafkaConsumer<>(props);

consumer.subscribe(Arrays.asList("topic"), 
    new ConsumerRebalanceListener() {
        
        @Override
        public void onPartitionsRevoked(
                Collection<TopicPartition> partitions) {
            // 分区被回收前调用
            // 保存消费位置
            for (TopicPartition partition : partitions) {
                offsetStore.save(partition, consumer.position(partition));
            }
            
            // 清理资源
            cleanupResources(partitions);
        }
        
        @Override
        public void onPartitionsAssigned(
                Collection<TopicPartition> partitions) {
            // 分区分配完成后调用
            // 恢复消费位置
            for (TopicPartition partition : partitions) {
                long offset = offsetStore.getOffset(partition);
                if (offset != null) {
                    consumer.seek(partition, offset);
                }
            }
            
            // 初始化消费
            initializeConsumption(partitions);
        }
    });
```

**再平衡问题与优化：**
```java
// 1. 再平衡风暴
// 多个消费者同时加入/离开导致频繁再平衡

// 解决方案：
// - 错开消费者启动时间
// - 增加 session.timeout.ms
// - 使用 StickyAssignor

// 2. 再平衡期间无法消费
// 解决方案：
// - 使用 CooperativeStickyAssignor
// - 增量再平衡，不停止消费

// 3. 分区分配不均
// 检查：
// - 主题分区数是否合理
// - 消费者数量是否合理
// - 分配策略是否合适
```

---

## 5. Kafka 高可用架构？

**题目：**
请详细说明 Kafka 的高可用架构设计。

**答案：**
**Kafka 集群架构：**
```
┌─────────────────────────────────────────────────────────────────┐
│                    Kafka 高可用架构                              │
│                                                              │
│  ┌─────────────────────────────────────────────────────────┐  │
│  │                    ZooKeeper                             │  │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐   │  │
│  │  │Broker 0 │  │Broker 1 │  │Broker 2 │  │Broker 3 │   │  │
│  │  │(Leader) │  │(Follower│  │(Follower│  │(Leader) │   │  │
│  │  │ Topic-0)│  │ Topic-0)│  │ Topic-1)│  │ Topic-1)│   │  │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘   │  │
│  │           │             │             │                │  │
│  │  ┌─────────┴─────────────┴─────────────┴─────────┐   │  │
│  │  │              Producer / Consumer                │   │  │
│  │  └─────────────────────────────────────────────────┘   │  │
│  │                                                          │
└─────────────────────────────────────────────────────────────────┘
```

**Controller 机制：**
```java
// Controller 是 Kafka 集群的核心组件
// 负责管理分区 Leader 选举、分区变化等

// Controller 选举（在 ZooKeeper 上）
void electController() {
    String path = "/controller";
    try {
        // 尝试创建临时节点
        String result = zkClient.createEphemeral(path, 
            currentBrokerId);
        
        // 创建成功，成为 Controller
        this.brokerId = currentBrokerId;
        this.isController = true;
        
        // 启动 Controller 服务
        startControllerService();
        
    } catch (NodeExistsException e) {
        // 节点已存在，获取当前 Controller
        String controllerId = zkClient.getData(path);
        if (controllerId.equals(currentBrokerId)) {
            // 自己就是 Controller
            this.isController = true;
        } else {
            // 等待新的 Controller
            this.isController = false;
            waitForControllerChange();
        }
    }
}

// Controller 职责
class ControllerService {
    
    // 1. 分区 Leader 选举
    void electLeaderForPartition(TopicPartition partition) {
        // 从 ISR 中选择新的 Leader
        List<Replica> isr = partition.isr;
        if (isr.isEmpty()) {
            // ISR 为空，等待
            return;
        }
        
        // 选择新的 Leader
        Replica newLeader = isr.get(0);
        
        // 更新 Zookeeper
        zkClient.updateLeaderForPartition(
            partition.topic, 
            partition.partition, 
            newLeader.brokerId);
        
        // 发送 LeaderAndIsr 请求
        sendLeaderAndIsrRequest(newLeader.brokerId, 
            partition, newLeader);
        
        // 发送 UpdateMetadata 请求
        broadcastUpdateMetadata(partition);
    }
    
    // 2. 处理 Broker 变化
    void handleBrokerChange(Broker broker, boolean isAlive) {
        if (isAlive) {
            // Broker 上线
            addReplicaToPartitions(broker);
        } else {
            // Broker 下线
            for (TopicPartition partition : broker.partitions) {
                electLeaderForPartition(partition);
            }
        }
    }
    
    // 3. 处理分区变化
    void handlePartitionChange(TopicPartition partition) {
        // 创建分区
        createPartition(partition);
        
        // 分配副本
        assignReplicasToPartitions(partition);
        
        // 选举 Leader
        electLeaderForPartition(partition);
    }
}
```

**Broker HA 设计：**
```java
// Broker 故障检测
class BrokerFailureDetector {
    
    // 1. Controller 检测 Broker 状态
    void monitorBrokerHealth() {
        // 定期检查 Broker 心跳
        for (Broker broker : allBrokers) {
            long lastHeartbeat = broker.lastHeartbeatTime;
            long now = System.currentTimeMillis();
            
            if (now - lastHeartbeat > brokerTimeout) {
                // Broker 超时，标记为离线
                handleBrokerFailure(broker);
            }
        }
    }
    
    // 2. Broker 下线处理
    void handleBrokerFailure(Broker broker) {
        // 1. 通知 Controller
        notifyController(broker.brokerId, false);
        
        // 2. 更新分区状态
        for (TopicPartition partition : broker.leaderPartitions) {
            // 该分区的 Leader 下线，需要重新选举
            controller.electLeaderForPartition(partition);
        }
        
        // 3. 副本切换
        for (TopicPartition partition : broker.replicaPartitions) {
            // 将该 Broker 从 ISR 中移除
            partition.removeReplica(broker);
        }
    }
}
```

**Leader 选举流程：**
```
┌─────────────────────────────────────────────────────────────────┐
│                    Leader 选举流程                               │
│                                                              │
│  1. Broker 下线检测                                             │
│     ├── Controller 检测到 Broker 超时                            │
│     └── 标记 Broker 为离线                                      │
│                                                              │
│  2. 触发分区 Leader 选举                                        │
│     ├── Controller 遍历该 Broker 负责的所有分区                  │
│     └── 对每个分区调用 electLeader                              │
│                                                              │
│  3. 从 ISR 中选举 Leader                                        │
│     ├── 检查分区 ISR 列表                                        │
│     ├── 如果 ISR 非空：从 ISR 中选择第一个 Broker                │
│     └── 如果 ISR 为空：等待 ISR 恢复（不允许选举）                │
│                                                              │
│  4. 更新 Zookeeper                                              │
│     ├── 更新分区 Leader 信息                                     │
│     └── 更新 ISR 列表                                           │
│                                                              │
│  5. 发送 LeaderAndIsr 请求                                      │
│     ├── 通知新 Leader 成为 Leader                                │
│     ├── 通知 Follower 变为 Follower                              │
│     └── 同步数据                                                │
│                                                              │
│  6. 广播 UpdateMetadata 请求                                     │
│     ├── 通知所有 Broker 分区 Leader 变化                         │
│     └── 消费者可以开始向新 Leader 拉取数据                       │
│                                                              │
└─────────────────────────────────────────────────────────────────┘
```

**高可用配置：**
```bash
# Broker 配置
broker.id=0
listeners=PLAINTEXT://0.0.0.0:9092
log.dirs=/tmp/kafka-logs
num.replica.fetchers=4
auto.leader.rebalance.enable=true
leader.imbalance.check.interval.seconds=300

# Controller 配置
controller.quorum.voters=0@localhost:9091,1@localhost:9092,2@localhost:9093
controller.listener.names=CONTROLLER
zookeeper.connect=localhost:2181

# 生产者配置
acks=all
retries=3
enable.idempotence=true

# 消费者配置
isolation.level=read_committed
session.timeout
## 答案
